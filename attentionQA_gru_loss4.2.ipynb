{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GR0CTPcu0B4R",
        "outputId": "16fa71ff-25d6-4c7d-e36f-7519b06a39ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-08-13 23:31:56--  https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\n",
            "Resolving rajpurkar.github.io (rajpurkar.github.io)... 185.199.111.153, 185.199.109.153, 185.199.108.153, ...\n",
            "Connecting to rajpurkar.github.io (rajpurkar.github.io)|185.199.111.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 42123633 (40M) [application/json]\n",
            "Saving to: ‘train-v2.0.json’\n",
            "\n",
            "train-v2.0.json     100%[===================>]  40.17M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2022-08-13 23:32:00 (377 MB/s) - ‘train-v2.0.json’ saved [42123633/42123633]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import unicodedata\n",
        "import string\n",
        "!wget https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\n",
        "# file = json.loads(open(\"dev-v2.0.json\").read())\n",
        "def read_squad_json(file):\n",
        "    # read json file\n",
        "    file = json.loads(open(file).read())\n",
        "    #get context column and questions column\n",
        "    context_questions = pd.json_normalize(file,['data','paragraphs'])\n",
        "    #repeat context column for each question\n",
        "    idx = np.repeat(context_questions['context'].values, context_questions.qas.str.len())\n",
        "    #separate question and answer columns\n",
        "    dataframe = pd.json_normalize(file,record_path=['data','paragraphs','qas'])\n",
        "    #add context to each question\n",
        "    dataframe['context'] = idx\n",
        "    return dataframe\n",
        "data = read_squad_json(\"train-v2.0.json\")\n",
        "def get_text(row):\n",
        "    answer = ''\n",
        "    plausible_answers =''\n",
        "    if row['answers']:\n",
        "        answer = row['answers'][0]['text']\n",
        "    if row['plausible_answers']:\n",
        "        plausible_answers= row['plausible_answers'][0]['text']\n",
        "    return answer +plausible_answers\n",
        "def answer_as_text(dataframe):\n",
        "    dataframe['plausible_answers'].fillna('',inplace=True)\n",
        "    dataframe['answer_text'] = dataframe.apply(get_text,axis=1)\n",
        "    dataframe['answer_text'] = 'sss '+ dataframe['answer_text'] + ' eee'\n",
        "    return dataframe\n",
        "data = answer_as_text(data)[['question','context','answer_text']]\n",
        "\n",
        "\n",
        "def remove_punc(text):\n",
        "    return text.translate(str.maketrans('','',string.punctuation))#'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
        "def lower_case(text):\n",
        "    return text.lower()\n",
        "def remove_non_english(text):#remove non ascci word\n",
        "    return unicodedata.normalize('NFKD',text).encode('ascii','ignore').decode('utf-8','ignore')\n",
        "def remove_links(text):\n",
        "    return re.sub(r'https?://\\S+','',text)\n",
        "def remove_whitespaces(text):\n",
        "    return text.strip()\n",
        "\n",
        "def clean_text(text):\n",
        "    return remove_punc(lower_case(remove_non_english((remove_links(text)))))\n",
        "\n",
        "data['question'] = data['question'].apply(clean_text)\n",
        "data['context'] = data['context'].apply(clean_text)\n",
        "data['answer_text'] = data['answer_text'].apply(clean_text)\n",
        "\n",
        "data['q_c']='CLS '+data['question']+' SEP '+data['context']\n",
        "\n",
        "pd.to_pickle(data,'data.pkl')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8WBKldHz2io"
      },
      "source": [
        "# import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "tf0qnZ-Hz2ip"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "from tensorflow.data import Dataset\n",
        "from tensorflow.keras.metrics import SparseCategoricalAccuracy\n",
        "import matplotlib.pyplot as plt\n",
        "import typing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JqoD50l_z2iq"
      },
      "outputs": [],
      "source": [
        "MAX_CONTEXT = 200\n",
        "MAX_ANSWER = 10\n",
        "MAX_VOC_SIZE = 20000\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "# word vector embedding size\n",
        "EMBEDDING_SIZE = 256\n",
        "# state size of LSTM\n",
        "STATE_SIZE = 1024\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWHIla5Oz2iq"
      },
      "source": [
        "# load pickle data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xYdOjh8Bz2iq"
      },
      "outputs": [],
      "source": [
        "data = pd.read_pickle(\"data.pkl\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "beG8Shmaz2ir",
        "outputId": "1d17ddf1-651e-43bb-d69e-43da43ac48db"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                  question  \\\n",
              "0  when did beyonce start becoming popular   \n",
              "\n",
              "                                             context  \\\n",
              "0  beyonce giselle knowlescarter bijnse beeyonsay...   \n",
              "\n",
              "                 answer_text  \\\n",
              "0  sss in the late 1990s eee   \n",
              "\n",
              "                                                 q_c  \n",
              "0  CLS when did beyonce start becoming popular SE...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d71eb1ca-3284-4525-94ee-a24912a8d8d5\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>context</th>\n",
              "      <th>answer_text</th>\n",
              "      <th>q_c</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>when did beyonce start becoming popular</td>\n",
              "      <td>beyonce giselle knowlescarter bijnse beeyonsay...</td>\n",
              "      <td>sss in the late 1990s eee</td>\n",
              "      <td>CLS when did beyonce start becoming popular SE...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d71eb1ca-3284-4525-94ee-a24912a8d8d5')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d71eb1ca-3284-4525-94ee-a24912a8d8d5 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d71eb1ca-3284-4525-94ee-a24912a8d8d5');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "data.head(1)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['q_c'].iloc[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "iAlgTHZmO78o",
        "outputId": "698914eb-1b9c-482c-dfb8-62e773d45cef"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'CLS when did beyonce start becoming popular SEP beyonce giselle knowlescarter bijnse beeyonsay born september 4 1981 is an american singer songwriter record producer and actress born and raised in houston texas she performed in various singing and dancing competitions as a child and rose to fame in the late 1990s as lead singer of rb girlgroup destinys child managed by her father mathew knowles the group became one of the worlds bestselling girl groups of all time their hiatus saw the release of beyonces debut album dangerously in love 2003 which established her as a solo artist worldwide earned five grammy awards and featured the billboard hot 100 numberone singles crazy in love and baby boy'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rD0Vvcrqz2ir"
      },
      "source": [
        "# build text vectorizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "5Z8GZUduz2ir"
      },
      "outputs": [],
      "source": [
        "input_tokenizer = TextVectorization(\n",
        "    standardize=None, output_sequence_length=MAX_CONTEXT, max_tokens=MAX_VOC_SIZE\n",
        ")\n",
        "output_tokenizer = TextVectorization(\n",
        "    standardize=None, output_sequence_length=MAX_ANSWER, max_tokens=MAX_VOC_SIZE\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "SN2XWtl2z2is"
      },
      "outputs": [],
      "source": [
        "input_tokenizer.adapt(data[\"q_c\"])\n",
        "output_tokenizer.adapt(data[\"answer_text\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GJzdkeWz2is"
      },
      "source": [
        "# create dataset iterator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "hREBlXcQz2is"
      },
      "outputs": [],
      "source": [
        "dataset = Dataset.from_tensor_slices((data[\"q_c\"], data[\"answer_text\"]))\n",
        "dataset = dataset.batch(BATCH_SIZE)\n",
        "\n",
        "# train = dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZCIis-zEz2is",
        "outputId": "e80d7b39-34ae-46f0-ef49-81c7c09c0fb6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(128,)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "dataset.as_numpy_iterator().next()[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_dROpDlez2is",
        "outputId": "092bbd9b-28d4-4128-bad9-77f6f174084b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 10), dtype=int64, numpy=\n",
              "array([[  12,   49,   46,  759,  710, 1042,  306,   11,  759,    1],\n",
              "       [  12,   20,  196,   46,  759, 3076,    5,   49,  209,   10],\n",
              "       [  12,   49,   46,  759, 2186, 6115,  953,    4,  212,    7]])>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# example_input_batch batch of sentences\n",
        "# example_tokens batch of tokens (sequences of words) (64, 21)\n",
        "example_tokens = input_tokenizer(dataset.as_numpy_iterator().next()[0])\n",
        "example_tokens[:3, :10]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "FdP6nqNoz2it",
        "outputId": "33105622-6e35-44b2-8bff-7922971750aa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'CLS when did beyonce start becoming popular SEP beyonce [UNK] [UNK] [UNK] [UNK] born september 4 1981 is an american singer songwriter record producer and actress born and raised in houston texas she performed in various singing and dancing competitions as a child and rose to fame in the late 1990s as lead singer of rb [UNK] destinys child managed by her father mathew knowles the group became one of the worlds bestselling girl groups of all time their hiatus saw the release of beyonces debut album dangerously in love 2003 which established her as a solo artist worldwide earned five grammy awards and featured the billboard hot 100 numberone singles crazy in love and baby boy                                                                                   '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "input_vocab = np.array(input_tokenizer.get_vocabulary())\n",
        "\n",
        "tokens = input_vocab[example_tokens[0].numpy()]\n",
        "' '.join(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SwaUHZfz2it"
      },
      "source": [
        "### Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "WO3-owg9z2it"
      },
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, input_vocab_size, embedding_dim, enc_units):\n",
        "        super().__init__()\n",
        "        self.enc_units = enc_units\n",
        "        self.input_vocab_size = input_vocab_size\n",
        "\n",
        "        # The embedding layer converts tokens to vectors\n",
        "        self.embedding = tf.keras.layers.Embedding(\n",
        "            self.input_vocab_size, embedding_dim\n",
        "        )  # ('batch_size', 'max_tokens in sentence', 'word vector size')\n",
        "\n",
        "        # The GRU RNN layer processes those vectors sequentially.\n",
        "        self.gru = tf.keras.layers.GRU(\n",
        "            self.enc_units,\n",
        "            # Return the sequence and state\n",
        "            return_sequences=True,\n",
        "            return_state=True,\n",
        "            recurrent_initializer=\"glorot_uniform\",\n",
        "        )\n",
        "\n",
        "    # override call method\n",
        "    # The call method runs the model\n",
        "    def call(self, tokens, state=None):\n",
        "        \"\"\"\n",
        "        tokens: input sequence in shape (batch_size, sequence_length) **question_context**\n",
        "        state: initial state of the RNN GRU layer\n",
        "        output:\n",
        "            output of the GRU layer in shape (batch_size, sequence_length, units)\n",
        "            state of the GRU layer in shape (batch_size, units)\n",
        "\n",
        "        \"\"\"\n",
        "        vectors = self.embedding(\n",
        "            tokens\n",
        "        )  # take the token and return the empedding vector\n",
        "\n",
        "        output, state = self.gru(\n",
        "            vectors, initial_state=state\n",
        "        )  # pass the embedding vector to the GRU layer\n",
        "\n",
        "        return output, state\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7sutmXSz2it"
      },
      "source": [
        "### Attention Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "1DPJ9UuRz2it"
      },
      "outputs": [],
      "source": [
        "# addaptive attention layer\n",
        "# tanh(w1@ encoder heddin state + w2 @ decoder heddin state)\n",
        "# will create two dense layer  as w1,w2\n",
        "\n",
        "\n",
        "class Attention(tf.keras.layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        super().__init__()\n",
        "\n",
        "        self.W1 = tf.keras.layers.Dense(units, use_bias=False)\n",
        "        self.W2 = tf.keras.layers.Dense(units, use_bias=False)\n",
        "\n",
        "        self.attention = tf.keras.layers.AdditiveAttention()\n",
        "\n",
        "    def call(self, query, value, mask):\n",
        "        \"\"\"query  :encoder heddin state\n",
        "        value :decoder heddin state This Will be the output of the encoder.\n",
        "        mask :mask of the decoder heddin state\n",
        "        output:\n",
        "            context vector of shape (batch_size, query_seq_length, units)\n",
        "            attention weights of shape  (batch_size, query_seq_length, value_seq_length)\n",
        "        \"\"\"\n",
        "        # W1@ht\n",
        "        w1_query = self.W1(query)\n",
        "\n",
        "        # W2@hs\n",
        "        w2_key = self.W2(value)\n",
        "\n",
        "        query_mask = tf.ones(\n",
        "            tf.shape(query)[:-1], dtype=bool\n",
        "        )  # [batch_size, query]. output will be zero at the positions where mask==False\n",
        "        value_mask = mask  # [batch_size, value] output will be zero at the positions where mask==False\n",
        "\n",
        "        context_vector, attention_weights = self.attention(\n",
        "            inputs=[w1_query, value, w2_key],\n",
        "            mask=[query_mask, value_mask],\n",
        "            return_attention_scores=True,\n",
        "        )\n",
        "\n",
        "        return context_vector, attention_weights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "I_wPfJk2z2iu"
      },
      "outputs": [],
      "source": [
        "# defin decoder input and output\n",
        "class DecoderInput(typing.NamedTuple):\n",
        "    new_tokens: typing.Any\n",
        "    enc_output: typing.Any\n",
        "    mask: typing.Any\n",
        "\n",
        "\n",
        "class DecoderOutput(typing.NamedTuple):\n",
        "    logits: typing.Any\n",
        "    attention_weights: typing.Any\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Yi1DyvChz2iu"
      },
      "outputs": [],
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, output_vocab_size, embedding_dim, dec_units):\n",
        "        super().__init__()\n",
        "        \"\"\"\n",
        "    output_vocab_size: size of the output vocabulary\n",
        "    embedding_dim: size of the embedding vector\n",
        "    dec_units: size of the hidden state of the GRU layer\n",
        "    \"\"\"\n",
        "        self.dec_units = dec_units\n",
        "        self.output_vocab_size = output_vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "\n",
        "        # The embedding layer converts token IDs to vectors\n",
        "        self.embedding = tf.keras.layers.Embedding(\n",
        "            self.output_vocab_size, embedding_dim\n",
        "        )\n",
        "\n",
        "        # The RNN keeps track of what's been generated so far.\n",
        "        self.gru = tf.keras.layers.GRU(\n",
        "            self.dec_units,\n",
        "            return_sequences=True,\n",
        "            return_state=True,\n",
        "            recurrent_initializer=\"glorot_uniform\",\n",
        "        )\n",
        "\n",
        "        # The RNN output will be the query for the attention layer.\n",
        "        self.attention = Attention(self.dec_units)\n",
        "\n",
        "        # context and decoder output will be combined to create the final output.\n",
        "        self.Wc = tf.keras.layers.Dense(\n",
        "            dec_units, activation=tf.math.tanh, use_bias=False\n",
        "        )\n",
        "\n",
        "        # produces the logits for each output token.\n",
        "        self.fc = tf.keras.layers.Dense(self.output_vocab_size)\n",
        "\n",
        "    def call(\n",
        "        self, inputs: DecoderInput, state=None\n",
        "    ) -> typing.Tuple[DecoderOutput, tf.Tensor]:\n",
        "        \"\"\"\n",
        "        inputs:\n",
        "            new_tokens: shape (batch_size, 1)\n",
        "            enc_output: shape (batch_size, sequence_length, hidden_size)\n",
        "            mask: shape (batch_size, sequence_length)\n",
        "        output:\n",
        "            logits: shape  (batch_size, t, output_vocab_size)\n",
        "            attention_weights: shape (batch_size, 1, sequence_length)\n",
        "            state: shape (batch_size, hidden_size)\n",
        "        \"\"\"\n",
        "\n",
        "        vectors = self.embedding(\n",
        "            inputs.new_tokens\n",
        "        )  # shape (batch_size, 1, embedding_dim)\n",
        "\n",
        "        #  Process one step with the RNN\n",
        "        rnn_output, state = self.gru(\n",
        "            vectors, initial_state=state\n",
        "        )  # (64, 1, 1024) (batch, words, units)\n",
        "\n",
        "        # Use the RNN output as the query for the attention over the encoder output\n",
        "        context_vector, attention_weights = self.attention(\n",
        "            query=rnn_output, value=inputs.enc_output, mask=inputs.mask\n",
        "        )\n",
        "\n",
        "        # Join the context_vector and rnn_output\n",
        "        context_and_rnn_output = tf.concat(\n",
        "            [context_vector, rnn_output], axis=-1\n",
        "        )  # shape: (batch t, value_units + query_units)\n",
        "\n",
        "        # at = tanh(Wc@[ct; ht])` #ct: context_vector, ht: rnn_output\n",
        "        attention_vector = self.Wc(context_and_rnn_output)\n",
        "\n",
        "        # Generate logit predictions: ('batch', 't', 'output_vocab_size')\n",
        "        logits = self.fc(attention_vector)\n",
        "\n",
        "        return DecoderOutput(logits, attention_weights), state\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Hm2Ce26z2iv"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "k6T_nqAtz2iv"
      },
      "outputs": [],
      "source": [
        "# loss function\n",
        "class MaskedLoss(tf.keras.losses.Loss):\n",
        "    def __init__(self):\n",
        "        self.name = \"masked_loss\"\n",
        "        self.loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "            from_logits=True, reduction=\"none\"\n",
        "        )\n",
        "\n",
        "    def __call__(self, y_true, y_pred):\n",
        "        # Calculate the loss for each item in the batch.\n",
        "        loss = self.loss(y_true, y_pred)\n",
        "\n",
        "        # Mask off the losses on padding.\n",
        "        mask = tf.cast(y_true != 0, tf.float32)\n",
        "        loss *= mask\n",
        "\n",
        "        # Return the total.\n",
        "        return tf.reduce_sum(loss)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFUaiR1bz2iv"
      },
      "source": [
        "### train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "DHWKCyiiz2iv"
      },
      "outputs": [],
      "source": [
        "class TrainTranslator(tf.keras.Model):\n",
        "    def __init__(\n",
        "        self,\n",
        "        embedding_dim,\n",
        "        units,\n",
        "        input_tokenizer_opject,\n",
        "        output_tokenizer_opject,\n",
        "        use_tf_function=True,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        # Build the encoder and decoder\n",
        "        self.encoder = Encoder(\n",
        "            input_tokenizer_opject.vocabulary_size(), embedding_dim, units\n",
        "        )\n",
        "        self.decoder = Decoder(\n",
        "            output_tokenizer_opject.vocabulary_size(), embedding_dim, units\n",
        "        )\n",
        "\n",
        "        self.input_tokenizer_opject = input_tokenizer_opject\n",
        "        self.output_tokenizer_opject = output_tokenizer_opject\n",
        "        self.use_tf_function = use_tf_function\n",
        "\n",
        "    def train_step(self, inputs):\n",
        "        if self.use_tf_function:\n",
        "            return self._tf_train_step(inputs)\n",
        "        else:\n",
        "            return self._train_step(inputs)\n",
        "\n",
        "    def _preprocess(self, input_text, target_text):\n",
        "        # Convert the text to token IDs\n",
        "        input_tokens = self.input_tokenizer_opject(input_text)\n",
        "        target_tokens = self.output_tokenizer_opject(target_text)\n",
        "\n",
        "        # Convert IDs to masks.\n",
        "        input_mask = input_tokens != 0\n",
        "\n",
        "        target_mask = target_tokens != 0\n",
        "\n",
        "        return input_tokens, input_mask, target_tokens, target_mask\n",
        "\n",
        "    def _train_step(self, inputs):\n",
        "        input_text, target_text = inputs\n",
        "\n",
        "        (input_tokens, input_mask, target_tokens, target_mask) = self._preprocess(\n",
        "            input_text, target_text\n",
        "        )\n",
        "\n",
        "        max_target_length = tf.shape(target_tokens)[1]  # number of output tokens\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            # Encode the input\n",
        "            enc_output, enc_state = self.encoder(input_tokens)\n",
        "\n",
        "            # Initialize the decoder's state to the encoder's final state.\n",
        "            # This only works if the encoder and decoder have the same number of\n",
        "            # units.\n",
        "            dec_state = enc_state\n",
        "            loss = tf.constant(0.0)\n",
        "\n",
        "            for t in tf.range(max_target_length - 1):\n",
        "                # Pass in two tokens from the target sequence:\n",
        "                # 1. The current input to the decoder.\n",
        "                # 2. The target for the decoder's next prediction.\n",
        "                new_tokens = target_tokens[:, t : t + 2]\n",
        "                step_loss, dec_state,y_pred = self._loop_step(\n",
        "                    new_tokens, input_mask, enc_output, dec_state\n",
        "                )\n",
        "                loss = loss + step_loss\n",
        "\n",
        "            # Average the loss over all non padding tokens.\n",
        "            average_loss = loss / tf.reduce_sum(tf.cast(target_mask, tf.float32))\n",
        "\n",
        "        # Compute gradients\n",
        "        variables = self.trainable_variables\n",
        "        gradients = tape.gradient(average_loss, variables)\n",
        "        # Update weights\n",
        "        self.optimizer.apply_gradients(zip(gradients, variables))\n",
        "        # Update the metrics.\n",
        "        # Metrics are configured in `compile()`.\n",
        "        self.compiled_metrics.update_state(new_tokens[:, 1:2], y_pred)\n",
        "\n",
        "        # Return a dict mapping metric names to current value\n",
        "        result = {\"batch_loss\": average_loss}\n",
        "        result.update({m.name: m.result() for m in self.metrics})\n",
        "        return result\n",
        "\n",
        "    def _loop_step(self, new_tokens, input_mask, enc_output, dec_state):\n",
        "\n",
        "        input_token, target_token = (\n",
        "            new_tokens[:, 0:1],\n",
        "            new_tokens[:, 1:2],\n",
        "        )  # first token is the input token, second token is the target token\n",
        "\n",
        "        # Run the decoder one step.\n",
        "        decoder_input = DecoderInput(\n",
        "            new_tokens=input_token, enc_output=enc_output, mask=input_mask\n",
        "        )\n",
        "\n",
        "        dec_result, dec_state = self.decoder(decoder_input, state=dec_state)\n",
        "\n",
        "        # self.loss returns the total for non-padded tokens\n",
        "        y = target_token\n",
        "        y_pred = dec_result.logits\n",
        "        step_loss = self.loss(\n",
        "            y, y_pred\n",
        "        )  # y: target_token (batch, 1), y_pred: logits (batch, output_vocab_size)\n",
        "\n",
        "        return step_loss, dec_state,y_pred\n",
        "\n",
        "    @tf.function(\n",
        "        input_signature=[\n",
        "            [\n",
        "                tf.TensorSpec(dtype=tf.string, shape=[None]),\n",
        "                tf.TensorSpec(dtype=tf.string, shape=[None]),\n",
        "            ]\n",
        "        ]\n",
        "    )\n",
        "    def _tf_train_step(self, inputs):\n",
        "        return self._train_step(inputs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "mBe-J4p_z2iv"
      },
      "outputs": [],
      "source": [
        "class BatchLogs(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, key):\n",
        "        self.key = key\n",
        "        self.logs = []\n",
        "\n",
        "    def on_train_batch_end(self, n, logs):\n",
        "        self.logs.append(logs[self.key])\n",
        "\n",
        "\n",
        "batch_loss = BatchLogs(\"batch_loss\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "GAmUc9ZRz2iv"
      },
      "outputs": [],
      "source": [
        "train_translator = TrainTranslator(\n",
        "    EMBEDDING_SIZE,\n",
        "    STATE_SIZE,\n",
        "    input_tokenizer_opject=input_tokenizer,\n",
        "    output_tokenizer_opject=output_tokenizer,\n",
        ")\n",
        "\n",
        "# Configure the loss and optimizer\n",
        "train_translator.compile(\n",
        "    optimizer=tf.optimizers.Adam(), loss=MaskedLoss(), metrics=[SparseCategoricalAccuracy()]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLbnQpTFz2iw",
        "outputId": "388f6d0c-8b39-4257-93a5-4c74033b393a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1019/1019 [==============================] - 982s 957ms/step - batch_loss: 5.4124 - sparse_categorical_accuracy: 0.0191\n",
            "Epoch 2/5\n",
            "1019/1019 [==============================] - 974s 956ms/step - batch_loss: 4.9016 - sparse_categorical_accuracy: 0.0206\n",
            "Epoch 3/5\n",
            "1019/1019 [==============================] - 976s 958ms/step - batch_loss: 4.5422 - sparse_categorical_accuracy: 0.0210\n",
            "Epoch 4/5\n",
            "1019/1019 [==============================] - 966s 948ms/step - batch_loss: 4.4570 - sparse_categorical_accuracy: 0.0217\n",
            "Epoch 5/5\n",
            "1019/1019 [==============================] - 965s 947ms/step - batch_loss: 4.2574 - sparse_categorical_accuracy: 0.0222\n"
          ]
        }
      ],
      "source": [
        "history = train_translator.fit(dataset, epochs=5, callbacks=[batch_loss])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "B5qzeoDkz2iw",
        "outputId": "a79b1e43-028c-4ed3-8d79-fe1d8db9b088"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'CE/token')"
            ]
          },
          "metadata": {},
          "execution_count": 27
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.plot(batch_loss.logs)\n",
        "plt.ylim([0, 3])\n",
        "plt.xlabel('Batch #')\n",
        "plt.ylabel('CE/token')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "VWzIGL8hz2iw"
      },
      "outputs": [],
      "source": [
        "class Translator(tf.Module):\n",
        "\n",
        "    def __init__(self, encoder, decoder, input_text_processor,\n",
        "                output_text_processor):\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.input_text_processor = input_text_processor\n",
        "        self.output_text_processor = output_text_processor\n",
        "\n",
        "        self.output_token_string_from_index = (\n",
        "            tf.keras.layers.StringLookup(\n",
        "                vocabulary=output_text_processor.get_vocabulary(),\n",
        "                mask_token='',\n",
        "                invert=True))\n",
        "\n",
        "        # The output should never generate padding, unknown, or start.\n",
        "        index_from_string = tf.keras.layers.StringLookup(\n",
        "            vocabulary=output_text_processor.get_vocabulary(), mask_token='')\n",
        "        token_mask_ids = index_from_string(['', '[UNK]', 'sss','CLS','SEP']).numpy()\n",
        "\n",
        "        token_mask = np.zeros([index_from_string.vocabulary_size()], dtype=np.bool)\n",
        "        token_mask[np.array(token_mask_ids)] = True\n",
        "        self.token_mask = token_mask\n",
        "\n",
        "        self.start_token = index_from_string(tf.constant('sss'))\n",
        "        self.end_token = index_from_string(tf.constant('eee'))\n",
        "    def tokens_to_text(self, result_tokens):\n",
        "\n",
        "        result_text_tokens = self.output_token_string_from_index(result_tokens)\n",
        "\n",
        "        result_text = tf.strings.reduce_join(result_text_tokens,\n",
        "                                            axis=1, separator=' ')\n",
        "\n",
        "        result_text = tf.strings.strip(result_text)\n",
        "        return result_text\n",
        "    def sample(self, logits, temperature):\n",
        "\n",
        "        token_mask = self.token_mask[tf.newaxis, tf.newaxis, :]#exlpand dimantion converet shape convert(5000,) to (1, 1, 5000)\n",
        "\n",
        "        print(token_mask.shape)\n",
        "        # Set the logits for all masked tokens to -inf, so they are never chosen.\n",
        "        logits = tf.where(self.token_mask, -np.inf, logits)\n",
        "\n",
        "        if temperature == 0.0:\n",
        "            new_tokens = tf.argmax(logits, axis=-1)\n",
        "        else:\n",
        "            logits = tf.squeeze(logits, axis=1)\n",
        "            new_tokens = tf.random.categorical(logits/temperature,\n",
        "                                                num_samples=1)\n",
        "        return new_tokens\n",
        "\n",
        "    def translate(self,input_text, *,max_length=10,return_attention=True,temperature=1.0):\n",
        "        batch_size = tf.shape(input_text)[0]\n",
        "        input_tokens = self.input_text_processor(input_text)\n",
        "        enc_output, enc_state = self.encoder(input_tokens)\n",
        "\n",
        "        dec_state = enc_state\n",
        "        new_tokens = tf.fill([batch_size, 1], self.start_token)# add start word foe eac sentence in batch\n",
        "\n",
        "        result_tokens = []\n",
        "        attention = []\n",
        "        done = tf.zeros([batch_size, 1], dtype=tf.bool)\n",
        "\n",
        "        for _ in range(max_length):\n",
        "            dec_input = DecoderInput(new_tokens=new_tokens,\n",
        "                                        enc_output=enc_output,\n",
        "                                        mask=(input_tokens!=0))\n",
        "\n",
        "            dec_result, dec_state = self.decoder(dec_input, state=dec_state) #dec_result : (logits,attention_weights)\n",
        "\n",
        "            attention.append(dec_result.attention_weights)\n",
        "\n",
        "            new_tokens = self.sample(dec_result.logits, temperature)\n",
        "\n",
        "            # If a sequence produces an `end_token`, set it `done` done = 0\n",
        "            done = done | (new_tokens == self.end_token) \n",
        "            # Once a sequence is done it only produces 0-padding.\n",
        "            new_tokens = tf.where(done, tf.constant(0, dtype=tf.int64), new_tokens)\n",
        "\n",
        "            # Collect the generated tokens\n",
        "            result_tokens.append(new_tokens)\n",
        "\n",
        "            if tf.executing_eagerly() and tf.reduce_all(done): #if there running excuation or all sentence in that word = 0 \n",
        "                break\n",
        "\n",
        "        # Convert the list of generates token ids to a list of strings.\n",
        "        result_tokens = tf.concat(result_tokens, axis=-1)\n",
        "        result_text = self.tokens_to_text(result_tokens)\n",
        "\n",
        "        if return_attention:\n",
        "            attention_stack = tf.concat(attention, axis=1)\n",
        "            return {'text': result_text, 'attention': attention_stack}\n",
        "        else:\n",
        "            return {'text': result_text}\n",
        "    @tf.function(input_signature=[tf.TensorSpec(dtype=tf.string, shape=[None])])\n",
        "    def tf_translate(self, input_text):\n",
        "        return self.translate(input_text)   "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "translator = Translator(\n",
        "    encoder=train_translator.encoder,\n",
        "    decoder=train_translator.decoder,\n",
        "    input_text_processor=input_tokenizer,\n",
        "    output_text_processor=output_tokenizer,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wz5dk7BdNz6E",
        "outputId": "4867341e-a03a-4579-c5c2-61d6441bfe66"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:21: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "input_text = tf.constant([\n",
        "    # 'hace mucho frio aqui.', # \"It's really cold here.\"\n",
        "    # 'Esta es mi vida.', # \"This is my life.\"\"\n",
        "    data['q_c'].iloc[0],\n",
        "    data['q_c'].iloc[0]\n",
        "    # data['answer_text'].iloc[0]\n",
        "])\n",
        "\n",
        "result = translator.translate(\n",
        "    input_text = input_text)\n",
        "\n",
        "print(result['text'][0].numpy().decode())\n",
        "print(result['text'][1].numpy().decode())\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3g9cgw9cOTGS",
        "outputId": "0e1f3370-8425-47fa-9974-cd48a7396622"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 1, 20000)\n",
            "(1, 1, 20000)\n",
            "(1, 1, 20000)\n",
            "a 1960s\n",
            "may 2015\n",
            "\n",
            "CPU times: user 74.3 ms, sys: 959 µs, total: 75.3 ms\n",
            "Wall time: 77.3 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['q_c'].iloc[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "2ezNxWirfDVz",
        "outputId": "ad7c3499-e237-4428-f23e-427760e72879"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'CLS when did beyonce start becoming popular SEP beyonce giselle knowlescarter bijnse beeyonsay born september 4 1981 is an american singer songwriter record producer and actress born and raised in houston texas she performed in various singing and dancing competitions as a child and rose to fame in the late 1990s as lead singer of rb girlgroup destinys child managed by her father mathew knowles the group became one of the worlds bestselling girl groups of all time their hiatus saw the release of beyonces debut album dangerously in love 2003 which established her as a solo artist worldwide earned five grammy awards and featured the billboard hot 100 numberone singles crazy in love and baby boy'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['answer_text'].iloc[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "-O8Wy4k2fJr7",
        "outputId": "a892d06b-467e-4ddc-930d-8f9a5f813ba5"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'sss in the late 1990s eee'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "BpQhmrjY2QXf"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "attentionQA.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.10.4 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}