{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GR0CTPcu0B4R",
    "outputId": "ac3fde59-ca90-4cd3-948a-993964514129"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-08-14 01:07:54--  https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\n",
      "Resolving rajpurkar.github.io (rajpurkar.github.io)... 185.199.108.153, 185.199.111.153, 185.199.109.153, ...\n",
      "Connecting to rajpurkar.github.io (rajpurkar.github.io)|185.199.108.153|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 42123633 (40M) [application/json]\n",
      "Saving to: ‘train-v2.0.json.1’\n",
      "\n",
      "train-v2.0.json.1   100%[===================>]  40.17M  --.-KB/s    in 0.1s    \n",
      "\n",
      "2022-08-14 01:08:58 (379 MB/s) - ‘train-v2.0.json.1’ saved [42123633/42123633]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "import string\n",
    "!wget https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\n",
    "# file = json.loads(open(\"dev-v2.0.json\").read())\n",
    "def read_squad_json(file):\n",
    "    # read json file\n",
    "    file = json.loads(open(file).read())\n",
    "    #get context column and questions column\n",
    "    context_questions = pd.json_normalize(file,['data','paragraphs'])\n",
    "    #repeat context column for each question\n",
    "    idx = np.repeat(context_questions['context'].values, context_questions.qas.str.len())\n",
    "    #separate question and answer columns\n",
    "    dataframe = pd.json_normalize(file,record_path=['data','paragraphs','qas'])\n",
    "    #add context to each question\n",
    "    dataframe['context'] = idx\n",
    "    return dataframe\n",
    "data = read_squad_json(\"train-v2.0.json\")\n",
    "def get_text(row):\n",
    "    answer = ''\n",
    "    plausible_answers =''\n",
    "    if row['answers']:\n",
    "        answer = row['answers'][0]['text']\n",
    "    if row['plausible_answers']:\n",
    "        plausible_answers= row['plausible_answers'][0]['text']\n",
    "    return answer +plausible_answers\n",
    "def answer_as_text(dataframe):\n",
    "    dataframe['plausible_answers'].fillna('',inplace=True)\n",
    "    dataframe['answer_text'] = dataframe.apply(get_text,axis=1)\n",
    "    dataframe['answer_text'] = 'sss '+ dataframe['answer_text'] + ' eee'\n",
    "    return dataframe\n",
    "data = answer_as_text(data)[['question','context','answer_text']]\n",
    "\n",
    "\n",
    "def remove_punc(text):\n",
    "    return text.translate(str.maketrans('','',string.punctuation))#'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
    "def lower_case(text):\n",
    "    return text.lower()\n",
    "def remove_non_english(text):#remove non ascci word\n",
    "    return unicodedata.normalize('NFKD',text).encode('ascii','ignore').decode('utf-8','ignore')\n",
    "def remove_links(text):\n",
    "    return re.sub(r'https?://\\S+','',text)\n",
    "def remove_whitespaces(text):\n",
    "    return text.strip()\n",
    "\n",
    "def clean_text(text):\n",
    "    return remove_punc(lower_case(remove_non_english((remove_links(text)))))\n",
    "\n",
    "data['question'] = data['question'].apply(clean_text)\n",
    "data['context'] = data['context'].apply(clean_text)\n",
    "data['answer_text'] = data['answer_text'].apply(clean_text)\n",
    "\n",
    "data['q_c']='CLS '+data['question']+' SEP '+data['context']\n",
    "\n",
    "pd.to_pickle(data,'data.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u8WBKldHz2io"
   },
   "source": [
    "# import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "tf0qnZ-Hz2ip"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from tensorflow.data import Dataset\n",
    "from tensorflow.keras.metrics import SparseCategoricalAccuracy\n",
    "import matplotlib.pyplot as plt\n",
    "import typing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "JqoD50l_z2iq"
   },
   "outputs": [],
   "source": [
    "MAX_CONTEXT = 350\n",
    "MAX_ANSWER = 10\n",
    "MAX_VOC_SIZE = 20000\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# word vector embedding size\n",
    "EMBEDDING_SIZE = 256\n",
    "# state size of LSTM\n",
    "STATE_SIZE = 1024\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aWHIla5Oz2iq"
   },
   "source": [
    "# load pickle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "xYdOjh8Bz2iq"
   },
   "outputs": [],
   "source": [
    "data = pd.read_pickle(\"data.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81
    },
    "id": "beG8Shmaz2ir",
    "outputId": "a38eee13-f707-4ea1-eb2b-4a6d5d5dee47"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>context</th>\n",
       "      <th>answer_text</th>\n",
       "      <th>q_c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>when did beyonce start becoming popular</td>\n",
       "      <td>beyonce giselle knowlescarter bijnse beeyonsay...</td>\n",
       "      <td>sss in the late 1990s eee</td>\n",
       "      <td>CLS when did beyonce start becoming popular SE...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  question  \\\n",
       "0  when did beyonce start becoming popular   \n",
       "\n",
       "                                             context  \\\n",
       "0  beyonce giselle knowlescarter bijnse beeyonsay...   \n",
       "\n",
       "                 answer_text  \\\n",
       "0  sss in the late 1990s eee   \n",
       "\n",
       "                                                 q_c  \n",
       "0  CLS when did beyonce start becoming popular SE...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "id": "iAlgTHZmO78o",
    "outputId": "231ca2a7-473a-4300-8110-6aba4a78d6bf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CLS in what country is normandy located SEP the normans norman nourmands french normands latin normanni were the people who in the 10th and 11th centuries gave their name to normandy a region in france they were descended from norse norman comes from norseman raiders and pirates from denmark iceland and norway who under their leader rollo agreed to swear fealty to king charles iii of west francia through generations of assimilation and mixing with the native frankish and romangaulish populations their descendants would gradually merge with the carolingianbased cultures of west francia the distinct cultural and ethnic identity of the normans emerged initially in the first half of the 10th century and it continued to evolve over the succeeding centuries'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['q_c'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rD0Vvcrqz2ir"
   },
   "source": [
    "# build text vectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "5Z8GZUduz2ir"
   },
   "outputs": [],
   "source": [
    "input_tokenizer = TextVectorization(\n",
    "    standardize=None, output_sequence_length=MAX_CONTEXT, max_tokens=MAX_VOC_SIZE\n",
    ")\n",
    "output_tokenizer = TextVectorization(\n",
    "    standardize=None, output_sequence_length=MAX_ANSWER, max_tokens=MAX_VOC_SIZE\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "SN2XWtl2z2is"
   },
   "outputs": [],
   "source": [
    "input_tokenizer.adapt(data[\"q_c\"])\n",
    "output_tokenizer.adapt(data[\"answer_text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-GJzdkeWz2is"
   },
   "source": [
    "# create dataset iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "hREBlXcQz2is"
   },
   "outputs": [],
   "source": [
    "dataset = Dataset.from_tensor_slices((data[\"q_c\"], data[\"answer_text\"]))\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "\n",
    "# train = dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZCIis-zEz2is",
    "outputId": "945f1efb-53ef-4350-829d-d45bd5c6f98f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.as_numpy_iterator().next()[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_dROpDlez2is",
    "outputId": "f734801f-9b4e-40c8-88b3-16740d16d4e2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 10), dtype=int64, numpy=\n",
       "array([[  12,   49,   46,  759,  710, 1042,  306,   11,  759,    1],\n",
       "       [  12,   20,  196,   46,  759, 3076,    5,   49,  209,   10],\n",
       "       [  12,   49,   46,  759, 2186, 6115,  953,    4,  212,    7]])>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example_input_batch batch of sentences\n",
    "# example_tokens batch of tokens (sequences of words) (64, 21)\n",
    "example_tokens = input_tokenizer(dataset.as_numpy_iterator().next()[0])\n",
    "example_tokens[:3, :10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "id": "FdP6nqNoz2it",
    "outputId": "cfae0e12-1d45-40fc-bfaa-b098f251affc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CLS when did beyonce start becoming popular SEP beyonce [UNK] [UNK] [UNK] [UNK] born september 4 1981 is an american singer songwriter record producer and actress born and raised in houston texas she performed in various singing and dancing competitions as a child and rose to fame in the late 1990s as lead singer of rb [UNK] destinys child managed by her father mathew knowles the group became one of the worlds bestselling girl groups of all time their hiatus saw the release of beyonces debut album dangerously in love 2003 which established her as a solo artist worldwide earned five grammy awards and featured the billboard hot 100 numberone singles crazy in love and baby boy                                                                                                                                                                                                                                         '"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_vocab = np.array(input_tokenizer.get_vocabulary())\n",
    "\n",
    "tokens = input_vocab[example_tokens[0].numpy()]\n",
    "' '.join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3SwaUHZfz2it"
   },
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    " tf.keras.layers.Embedding??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "WO3-owg9z2it"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, input_vocab_size, embedding_dim, enc_units):\n",
    "        super().__init__()\n",
    "        self.enc_units = enc_units\n",
    "        self.input_vocab_size = input_vocab_size\n",
    "\n",
    "        # The embedding layer converts tokens to vectors\n",
    "        self.embedding = tf.keras.layers.Embedding(\n",
    "            self.input_vocab_size, embedding_dim\n",
    "        )  # ('batch_size', 'max_tokens in sentence', 'word vector size')\n",
    "\n",
    "        # The GRU RNN layer processes those vectors sequentially.\n",
    "        self.lstm = tf.keras.layers.LSTM(\n",
    "            self.enc_units,\n",
    "            # Return the sequence and state\n",
    "            return_sequences=True,\n",
    "            return_state=True,\n",
    "            recurrent_initializer=\"glorot_uniform\",\n",
    "        )\n",
    "\n",
    "    # override call method\n",
    "    # The call method runs the model\n",
    "    def call(self, tokens, state=None):\n",
    "        \"\"\"\n",
    "        tokens: input sequence in shape (batch_size, sequence_length) **question_context**\n",
    "        state: initial state of the RNN GRU layer\n",
    "        output:\n",
    "            output of the GRU layer in shape (batch_size, sequence_length, units)\n",
    "            state of the GRU layer in shape (batch_size, units)\n",
    "\n",
    "        \"\"\"\n",
    "        vectors = self.embedding(\n",
    "            tokens\n",
    "        )  # take the token and return the empedding vector\n",
    "\n",
    "        whole_seq_output, final_memory_state, final_carry_state = self.lstm(\n",
    "            vectors, initial_state=state\n",
    "        )  # pass the embedding vector to the GRU layer\n",
    "\n",
    "        return whole_seq_output, [final_memory_state, final_carry_state ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T7sutmXSz2it"
   },
   "source": [
    "### Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "1DPJ9UuRz2it"
   },
   "outputs": [],
   "source": [
    "# addaptive attention layer\n",
    "# tanh(w1@ encoder heddin state + w2 @ decoder heddin state)\n",
    "# will create two dense layer  as w1,w2\n",
    "\n",
    "\n",
    "class Attention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super().__init__()\n",
    "\n",
    "        self.W1 = tf.keras.layers.Dense(units, use_bias=False)\n",
    "        self.W2 = tf.keras.layers.Dense(units, use_bias=False)\n",
    "\n",
    "        self.attention = tf.keras.layers.AdditiveAttention()\n",
    "\n",
    "    def call(self, query, value, mask):\n",
    "        \"\"\"query  :encoder heddin state\n",
    "        value :decoder heddin state This Will be the output of the encoder.\n",
    "        mask :mask of the decoder heddin state\n",
    "        output:\n",
    "            context vector of shape (batch_size, query_seq_length, units)\n",
    "            attention weights of shape  (batch_size, query_seq_length, value_seq_length)\n",
    "        \"\"\"\n",
    "        # W1@ht\n",
    "        w1_query = self.W1(query)\n",
    "\n",
    "        # W2@hs\n",
    "        w2_key = self.W2(value)\n",
    "\n",
    "        query_mask = tf.ones(\n",
    "            tf.shape(query)[:-1], dtype=bool\n",
    "        )  # [batch_size, query]. output will be zero at the positions where mask==False\n",
    "        value_mask = mask  # [batch_size, value] output will be zero at the positions where mask==False\n",
    "\n",
    "        context_vector, attention_weights = self.attention(\n",
    "            inputs=[w1_query, value, w2_key],\n",
    "            mask=[query_mask, value_mask],\n",
    "            return_attention_scores=True,\n",
    "        )\n",
    "\n",
    "        return context_vector, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "I_wPfJk2z2iu"
   },
   "outputs": [],
   "source": [
    "# defin decoder input and output\n",
    "class DecoderInput(typing.NamedTuple):\n",
    "    new_tokens: typing.Any\n",
    "    enc_output: typing.Any\n",
    "    mask: typing.Any\n",
    "\n",
    "\n",
    "class DecoderOutput(typing.NamedTuple):\n",
    "    logits: typing.Any\n",
    "    attention_weights: typing.Any\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "Yi1DyvChz2iu"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, output_vocab_size, embedding_dim, dec_units):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "    output_vocab_size: size of the output vocabulary\n",
    "    embedding_dim: size of the embedding vector\n",
    "    dec_units: size of the hidden state of the GRU layer\n",
    "    \"\"\"\n",
    "        self.dec_units = dec_units\n",
    "        self.output_vocab_size = output_vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        # The embedding layer converts token IDs to vectors\n",
    "        self.embedding = tf.keras.layers.Embedding(\n",
    "            self.output_vocab_size, embedding_dim\n",
    "        )\n",
    "\n",
    "        # The RNN keeps track of what's been generated so far.\n",
    "        self.lstm = tf.keras.layers.LSTM(\n",
    "            self.dec_units,\n",
    "            return_sequences=True,\n",
    "            return_state=True,\n",
    "            recurrent_initializer=\"glorot_uniform\",\n",
    "        )\n",
    "\n",
    "        # The RNN output will be the query for the attention layer.\n",
    "        self.attention = Attention(self.dec_units)\n",
    "\n",
    "        # context and decoder output will be combined to create the final output.\n",
    "        self.Wc = tf.keras.layers.Dense(\n",
    "            dec_units, activation=tf.math.tanh, use_bias=False\n",
    "        )\n",
    "\n",
    "        # produces the logits for each output token.\n",
    "        self.fc = tf.keras.layers.Dense(self.output_vocab_size)\n",
    "\n",
    "    def call(\n",
    "        self, inputs: DecoderInput, state=None\n",
    "    ) -> typing.Tuple[DecoderOutput, tf.Tensor]:\n",
    "        \"\"\"\n",
    "        inputs:\n",
    "            new_tokens: shape (batch_size, 1)\n",
    "            enc_output: shape (batch_size, sequence_length, hidden_size)\n",
    "            mask: shape (batch_size, sequence_length)\n",
    "        output:\n",
    "            logits: shape  (batch_size, t, output_vocab_size)\n",
    "            attention_weights: shape (batch_size, 1, sequence_length)\n",
    "            state: shape (batch_size, hidden_size)\n",
    "        \"\"\"\n",
    "\n",
    "        vectors = self.embedding(\n",
    "            inputs.new_tokens\n",
    "        )  # shape (batch_size, 1, embedding_dim)\n",
    "\n",
    "        #  Process one step with the RNN\n",
    "        whole_seq_output, final_memory_state, final_carry_state  = self.lstm(\n",
    "            vectors, initial_state=state\n",
    "        )  # (64, 1, 1024) (batch, words, units)\n",
    "\n",
    "        # Use the RNN output as the query for the attention over the encoder output\n",
    "        context_vector, attention_weights = self.attention(\n",
    "            query=whole_seq_output, value=inputs.enc_output, mask=inputs.mask\n",
    "        )\n",
    "\n",
    "        # Join the context_vector and rnn_output\n",
    "        context_and_rnn_output = tf.concat(\n",
    "            [context_vector, whole_seq_output], axis=-1\n",
    "        )  # shape: (batch t, value_units + query_units)\n",
    "\n",
    "        # at = tanh(Wc@[ct; ht])` #ct: context_vector, ht: rnn_output\n",
    "        attention_vector = self.Wc(context_and_rnn_output)\n",
    "\n",
    "        # Generate logit predictions: ('batch', 't', 'output_vocab_size')\n",
    "        logits = self.fc(attention_vector)\n",
    "\n",
    "        return DecoderOutput(logits, attention_weights), [final_memory_state, final_carry_state]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Hm2Ce26z2iv"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "k6T_nqAtz2iv"
   },
   "outputs": [],
   "source": [
    "# loss function\n",
    "class MaskedLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self):\n",
    "        self.name = \"masked_loss\"\n",
    "        self.loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "            from_logits=True, reduction=\"none\"\n",
    "        )\n",
    "\n",
    "    def __call__(self, y_true, y_pred):\n",
    "        # Calculate the loss for each item in the batch.\n",
    "        loss = self.loss(y_true, y_pred)\n",
    "\n",
    "        # Mask off the losses on padding.\n",
    "        mask = tf.cast(y_true != 0, tf.float32)\n",
    "        loss *= mask\n",
    "\n",
    "        # Return the total.\n",
    "        return tf.reduce_sum(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cFUaiR1bz2iv"
   },
   "source": [
    "### train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "DHWKCyiiz2iv"
   },
   "outputs": [],
   "source": [
    "class TrainTranslator(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim,\n",
    "        units,\n",
    "        input_tokenizer_opject,\n",
    "        output_tokenizer_opject,\n",
    "        use_tf_function=True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # Build the encoder and decoder\n",
    "        self.encoder = Encoder(\n",
    "            input_tokenizer_opject.vocabulary_size(), embedding_dim, units\n",
    "        )\n",
    "        self.decoder = Decoder(\n",
    "            output_tokenizer_opject.vocabulary_size(), embedding_dim, units\n",
    "        )\n",
    "\n",
    "        self.input_tokenizer_opject = input_tokenizer_opject\n",
    "        self.output_tokenizer_opject = output_tokenizer_opject\n",
    "        self.use_tf_function = use_tf_function\n",
    "        \n",
    "\n",
    "    def train_step(self, inputs):\n",
    "        if self.use_tf_function:\n",
    "            return self._tf_train_step(inputs)\n",
    "        else:\n",
    "            return self._train_step(inputs)\n",
    "\n",
    "    def _preprocess(self, input_text, target_text):\n",
    "        # Convert the text to token IDs\n",
    "        input_tokens = self.input_tokenizer_opject(input_text)\n",
    "        target_tokens = self.output_tokenizer_opject(target_text)\n",
    "\n",
    "        # Convert IDs to masks.\n",
    "        input_mask = input_tokens != 0\n",
    "\n",
    "        target_mask = target_tokens != 0\n",
    "\n",
    "        return input_tokens, input_mask, target_tokens, target_mask\n",
    "\n",
    "    def _train_step(self, inputs):\n",
    "        input_text, target_text = inputs\n",
    "\n",
    "        (input_tokens, input_mask, target_tokens, target_mask) = self._preprocess(\n",
    "            input_text, target_text\n",
    "        )\n",
    "\n",
    "        max_target_length = tf.shape(target_tokens)[1]  # number of output tokens\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Encode the input\n",
    "            enc_output, enc_state = self.encoder(input_tokens)\n",
    "\n",
    "            # Initialize the decoder's state to the encoder's final state.\n",
    "            # This only works if the encoder and decoder have the same number of\n",
    "            # units.\n",
    "            dec_state = enc_state\n",
    "            loss = tf.constant(0.0)\n",
    "\n",
    "            for t in tf.range(max_target_length - 1):\n",
    "                # Pass in two tokens from the target sequence:\n",
    "                # 1. The current input to the decoder.\n",
    "                # 2. The target for the decoder's next prediction.\n",
    "                new_tokens = target_tokens[:, t : t + 2]\n",
    "                step_loss, dec_state,y_pred = self._loop_step(\n",
    "                    new_tokens, input_mask, enc_output, dec_state\n",
    "                )\n",
    "                loss = loss + step_loss\n",
    "                self.compiled_metrics.update_state(new_tokens[:, 1:2], y_pred)\n",
    "\n",
    "            # Average the loss over all non padding tokens.\n",
    "            average_loss = loss / tf.reduce_sum(tf.cast(target_mask, tf.float32))\n",
    "\n",
    "        # Compute gradients\n",
    "        variables = self.trainable_variables\n",
    "        gradients = tape.gradient(average_loss, variables)\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, variables))\n",
    "        # Update the metrics.\n",
    "        # Metrics are configured in `compile()`.\n",
    "\n",
    "        # Return a dict mapping metric names to current value\n",
    "        result = {\"batch_loss\": average_loss}\n",
    "        result.update({m.name: m.result() for m in self.metrics})\n",
    "        return result\n",
    "\n",
    "    def _loop_step(self, new_tokens, input_mask, enc_output, dec_state):\n",
    "\n",
    "        input_token, target_token = (\n",
    "            new_tokens[:, 0:1],\n",
    "            new_tokens[:, 1:2],\n",
    "        )  # first token is the input token, second token is the target token\n",
    "\n",
    "        # Run the decoder one step.\n",
    "        decoder_input = DecoderInput(\n",
    "            new_tokens=input_token, enc_output=enc_output, mask=input_mask\n",
    "        )\n",
    "\n",
    "        dec_result, dec_state = self.decoder(decoder_input, state=dec_state)\n",
    "\n",
    "        # self.loss returns the total for non-padded tokens\n",
    "        y = target_token\n",
    "        y_pred = dec_result.logits\n",
    "        step_loss = self.loss(\n",
    "            y, y_pred\n",
    "        )  # y: target_token (batch, 1), y_pred: logits (batch, output_vocab_size)\n",
    "\n",
    "        return step_loss, dec_state,y_pred\n",
    "\n",
    "    @tf.function(\n",
    "        input_signature=[\n",
    "            [\n",
    "                tf.TensorSpec(dtype=tf.string, shape=[None]),\n",
    "                tf.TensorSpec(dtype=tf.string, shape=[None]),\n",
    "            ]\n",
    "        ]\n",
    "    )\n",
    "    def _tf_train_step(self, inputs):\n",
    "        return self._train_step(inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "mBe-J4p_z2iv"
   },
   "outputs": [],
   "source": [
    "class BatchLogs(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "        self.logs = []\n",
    "\n",
    "    def on_train_batch_end(self, n, logs):\n",
    "        self.logs.append(logs[self.key])\n",
    "\n",
    "\n",
    "batch_loss = BatchLogs(\"batch_loss\")\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "GAmUc9ZRz2iv"
   },
   "outputs": [],
   "source": [
    "train_translator = TrainTranslator(\n",
    "    EMBEDDING_SIZE,\n",
    "    STATE_SIZE,\n",
    "    input_tokenizer_opject=input_tokenizer,\n",
    "    output_tokenizer_opject=output_tokenizer,\n",
    ")\n",
    "\n",
    "# Configure the loss and optimizer\n",
    "train_translator.compile(\n",
    "    optimizer=tf.optimizers.Adam(), loss=MaskedLoss(), metrics=[SparseCategoricalAccuracy()]\n",
    ")\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yLbnQpTFz2iw",
    "outputId": "31661d7b-8959-4f40-c3b5-c7ebd2a0c74d",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1019/1019 [==============================] - ETA: 0s - batch_loss: 5.0172 - sparse_categorical_accuracy: 0.1219WARNING:tensorflow:Early stopping conditioned on metric `loss` which is not available. Available metrics are: batch_loss,sparse_categorical_accuracy\n",
      "1019/1019 [==============================] - 985s 967ms/step - batch_loss: 5.0161 - sparse_categorical_accuracy: 0.1219\n",
      "Epoch 2/20\n",
      "1019/1019 [==============================] - ETA: 0s - batch_loss: 4.3396 - sparse_categorical_accuracy: 0.1282WARNING:tensorflow:Early stopping conditioned on metric `loss` which is not available. Available metrics are: batch_loss,sparse_categorical_accuracy\n",
      "1019/1019 [==============================] - 986s 968ms/step - batch_loss: 4.3384 - sparse_categorical_accuracy: 0.1282\n",
      "Epoch 3/20\n",
      "1019/1019 [==============================] - ETA: 0s - batch_loss: 3.9765 - sparse_categorical_accuracy: 0.1325WARNING:tensorflow:Early stopping conditioned on metric `loss` which is not available. Available metrics are: batch_loss,sparse_categorical_accuracy\n",
      "1019/1019 [==============================] - 994s 975ms/step - batch_loss: 3.9749 - sparse_categorical_accuracy: 0.1325\n",
      "Epoch 4/20\n",
      "1019/1019 [==============================] - ETA: 0s - batch_loss: 3.6174 - sparse_categorical_accuracy: 0.1377WARNING:tensorflow:Early stopping conditioned on metric `loss` which is not available. Available metrics are: batch_loss,sparse_categorical_accuracy\n",
      "1019/1019 [==============================] - 995s 977ms/step - batch_loss: 3.6158 - sparse_categorical_accuracy: 0.1377\n",
      "Epoch 5/20\n",
      "1019/1019 [==============================] - ETA: 0s - batch_loss: 3.2952 - sparse_categorical_accuracy: 0.1452WARNING:tensorflow:Early stopping conditioned on metric `loss` which is not available. Available metrics are: batch_loss,sparse_categorical_accuracy\n",
      "1019/1019 [==============================] - 995s 976ms/step - batch_loss: 3.2936 - sparse_categorical_accuracy: 0.1452\n",
      "Epoch 6/20\n",
      "1019/1019 [==============================] - ETA: 0s - batch_loss: 3.0070 - sparse_categorical_accuracy: 0.1558WARNING:tensorflow:Early stopping conditioned on metric `loss` which is not available. Available metrics are: batch_loss,sparse_categorical_accuracy\n",
      "1019/1019 [==============================] - 1001s 982ms/step - batch_loss: 3.0052 - sparse_categorical_accuracy: 0.1558\n",
      "Epoch 7/20\n",
      "1019/1019 [==============================] - ETA: 0s - batch_loss: 2.7329 - sparse_categorical_accuracy: 0.1696WARNING:tensorflow:Early stopping conditioned on metric `loss` which is not available. Available metrics are: batch_loss,sparse_categorical_accuracy\n",
      "1019/1019 [==============================] - 1039s 1s/step - batch_loss: 2.7312 - sparse_categorical_accuracy: 0.1696\n",
      "Epoch 8/20\n",
      "1019/1019 [==============================] - ETA: 0s - batch_loss: 2.5024 - sparse_categorical_accuracy: 0.1832WARNING:tensorflow:Early stopping conditioned on metric `loss` which is not available. Available metrics are: batch_loss,sparse_categorical_accuracy\n",
      "1019/1019 [==============================] - 987s 969ms/step - batch_loss: 2.5010 - sparse_categorical_accuracy: 0.1832\n",
      "Epoch 9/20\n",
      "1019/1019 [==============================] - ETA: 0s - batch_loss: 2.1742 - sparse_categorical_accuracy: 0.2051WARNING:tensorflow:Early stopping conditioned on metric `loss` which is not available. Available metrics are: batch_loss,sparse_categorical_accuracy\n",
      "1019/1019 [==============================] - 989s 970ms/step - batch_loss: 2.1725 - sparse_categorical_accuracy: 0.2051\n",
      "Epoch 10/20\n",
      "1019/1019 [==============================] - ETA: 0s - batch_loss: 1.8581 - sparse_categorical_accuracy: 0.2284WARNING:tensorflow:Early stopping conditioned on metric `loss` which is not available. Available metrics are: batch_loss,sparse_categorical_accuracy\n",
      "1019/1019 [==============================] - 989s 970ms/step - batch_loss: 1.8567 - sparse_categorical_accuracy: 0.2284\n",
      "Epoch 11/20\n",
      "1019/1019 [==============================] - ETA: 0s - batch_loss: 1.5925 - sparse_categorical_accuracy: 0.2502WARNING:tensorflow:Early stopping conditioned on metric `loss` which is not available. Available metrics are: batch_loss,sparse_categorical_accuracy\n",
      "1019/1019 [==============================] - 987s 968ms/step - batch_loss: 1.5914 - sparse_categorical_accuracy: 0.2502\n",
      "Epoch 12/20\n",
      "1019/1019 [==============================] - ETA: 0s - batch_loss: 1.3507 - sparse_categorical_accuracy: 0.2719WARNING:tensorflow:Early stopping conditioned on metric `loss` which is not available. Available metrics are: batch_loss,sparse_categorical_accuracy\n",
      "1019/1019 [==============================] - 987s 968ms/step - batch_loss: 1.3498 - sparse_categorical_accuracy: 0.2719\n",
      "Epoch 13/20\n",
      "1019/1019 [==============================] - ETA: 0s - batch_loss: 1.1320 - sparse_categorical_accuracy: 0.2926WARNING:tensorflow:Early stopping conditioned on metric `loss` which is not available. Available metrics are: batch_loss,sparse_categorical_accuracy\n",
      "1019/1019 [==============================] - 986s 968ms/step - batch_loss: 1.1311 - sparse_categorical_accuracy: 0.2926\n",
      "Epoch 14/20\n",
      "1019/1019 [==============================] - ETA: 0s - batch_loss: 0.9497 - sparse_categorical_accuracy: 0.3097WARNING:tensorflow:Early stopping conditioned on metric `loss` which is not available. Available metrics are: batch_loss,sparse_categorical_accuracy\n",
      "1019/1019 [==============================] - 987s 968ms/step - batch_loss: 0.9490 - sparse_categorical_accuracy: 0.3097\n",
      "Epoch 15/20\n",
      "1019/1019 [==============================] - ETA: 0s - batch_loss: 0.7955 - sparse_categorical_accuracy: 0.3248WARNING:tensorflow:Early stopping conditioned on metric `loss` which is not available. Available metrics are: batch_loss,sparse_categorical_accuracy\n",
      "1019/1019 [==============================] - 987s 969ms/step - batch_loss: 0.7950 - sparse_categorical_accuracy: 0.3248\n",
      "Epoch 16/20\n",
      "1019/1019 [==============================] - ETA: 0s - batch_loss: 0.6501 - sparse_categorical_accuracy: 0.3399WARNING:tensorflow:Early stopping conditioned on metric `loss` which is not available. Available metrics are: batch_loss,sparse_categorical_accuracy\n",
      "1019/1019 [==============================] - 987s 969ms/step - batch_loss: 0.6497 - sparse_categorical_accuracy: 0.3399\n",
      "Epoch 17/20\n",
      "1019/1019 [==============================] - ETA: 0s - batch_loss: 0.5250 - sparse_categorical_accuracy: 0.3536WARNING:tensorflow:Early stopping conditioned on metric `loss` which is not available. Available metrics are: batch_loss,sparse_categorical_accuracy\n",
      "1019/1019 [==============================] - 988s 969ms/step - batch_loss: 0.5246 - sparse_categorical_accuracy: 0.3536\n",
      "Epoch 18/20\n",
      "1019/1019 [==============================] - ETA: 0s - batch_loss: 0.4217 - sparse_categorical_accuracy: 0.3655WARNING:tensorflow:Early stopping conditioned on metric `loss` which is not available. Available metrics are: batch_loss,sparse_categorical_accuracy\n",
      "1019/1019 [==============================] - 991s 973ms/step - batch_loss: 0.4216 - sparse_categorical_accuracy: 0.3655\n",
      "Epoch 19/20\n",
      "1019/1019 [==============================] - ETA: 0s - batch_loss: 0.3459 - sparse_categorical_accuracy: 0.3753WARNING:tensorflow:Early stopping conditioned on metric `loss` which is not available. Available metrics are: batch_loss,sparse_categorical_accuracy\n",
      "1019/1019 [==============================] - 988s 970ms/step - batch_loss: 0.3456 - sparse_categorical_accuracy: 0.3753\n",
      "Epoch 20/20\n",
      "1019/1019 [==============================] - ETA: 0s - batch_loss: 0.2861 - sparse_categorical_accuracy: 0.3832WARNING:tensorflow:Early stopping conditioned on metric `loss` which is not available. Available metrics are: batch_loss,sparse_categorical_accuracy\n",
      "1019/1019 [==============================] - 989s 971ms/step - batch_loss: 0.2859 - sparse_categorical_accuracy: 0.3832\n"
     ]
    }
   ],
   "source": [
    "history = train_translator.fit(dataset, epochs=20, callbacks=[batch_loss,callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "B5qzeoDkz2iw",
    "outputId": "1e649535-fb0a-46bf-da2d-a6f870441201"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'CE/token')"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.plot(batch_loss.logs)\n",
    "plt.ylim([0, 20])\n",
    "plt.xlabel('Batch #')\n",
    "plt.ylabel('CE/token')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "VWzIGL8hz2iw"
   },
   "outputs": [],
   "source": [
    "class Translator(tf.Module):\n",
    "\n",
    "    def __init__(self, encoder, decoder, input_text_processor,\n",
    "                output_text_processor):\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.input_text_processor = input_text_processor\n",
    "        self.output_text_processor = output_text_processor\n",
    "\n",
    "        self.output_token_string_from_index = (\n",
    "            tf.keras.layers.StringLookup(\n",
    "                vocabulary=output_text_processor.get_vocabulary(),\n",
    "                mask_token='',\n",
    "                invert=True))\n",
    "\n",
    "        # The output should never generate padding, unknown, or start.\n",
    "        index_from_string = tf.keras.layers.StringLookup(\n",
    "            vocabulary=output_text_processor.get_vocabulary(), mask_token='')\n",
    "        token_mask_ids = index_from_string(['', '[UNK]', 'sss','CLS','SEP']).numpy()\n",
    "\n",
    "        token_mask = np.zeros([index_from_string.vocabulary_size()], dtype=np.bool)\n",
    "        token_mask[np.array(token_mask_ids)] = True\n",
    "        self.token_mask = token_mask\n",
    "\n",
    "        self.start_token = index_from_string(tf.constant('sss'))\n",
    "        self.end_token = index_from_string(tf.constant('eee'))\n",
    "    def tokens_to_text(self, result_tokens):\n",
    "\n",
    "        result_text_tokens = self.output_token_string_from_index(result_tokens)\n",
    "\n",
    "        result_text = tf.strings.reduce_join(result_text_tokens,\n",
    "                                            axis=1, separator=' ')\n",
    "\n",
    "        result_text = tf.strings.strip(result_text)\n",
    "        return result_text\n",
    "    def sample(self, logits, temperature):\n",
    "\n",
    "        token_mask = self.token_mask[tf.newaxis, tf.newaxis, :]#exlpand dimantion converet shape convert(5000,) to (1, 1, 5000)\n",
    "\n",
    "        print(token_mask.shape)\n",
    "        # Set the logits for all masked tokens to -inf, so they are never chosen.\n",
    "        logits = tf.where(self.token_mask, -np.inf, logits)\n",
    "\n",
    "        if temperature == 0.0:\n",
    "            new_tokens = tf.argmax(logits, axis=-1)\n",
    "        else:\n",
    "            logits = tf.squeeze(logits, axis=1)\n",
    "            new_tokens = tf.random.categorical(logits/temperature,\n",
    "                                                num_samples=1)\n",
    "        return new_tokens\n",
    "\n",
    "    def translate(self,input_text, *,max_length=10,return_attention=True,temperature=1.0):\n",
    "        batch_size = tf.shape(input_text)[0]\n",
    "        input_tokens = self.input_text_processor(input_text)\n",
    "        enc_output, enc_state = self.encoder(input_tokens)\n",
    "\n",
    "        dec_state = enc_state\n",
    "        new_tokens = tf.fill([batch_size, 1], self.start_token)# add start word foe eac sentence in batch\n",
    "\n",
    "        result_tokens = []\n",
    "        attention = []\n",
    "        done = tf.zeros([batch_size, 1], dtype=tf.bool)\n",
    "\n",
    "        for _ in range(max_length):\n",
    "            dec_input = DecoderInput(new_tokens=new_tokens,\n",
    "                                        enc_output=enc_output,\n",
    "                                        mask=(input_tokens!=0))\n",
    "\n",
    "            dec_result, dec_state = self.decoder(dec_input, state=dec_state) #dec_result : (logits,attention_weights)\n",
    "\n",
    "            attention.append(dec_result.attention_weights)\n",
    "\n",
    "            new_tokens = self.sample(dec_result.logits, temperature)\n",
    "\n",
    "            # If a sequence produces an `end_token`, set it `done` done = 0\n",
    "            done = done | (new_tokens == self.end_token) \n",
    "            # Once a sequence is done it only produces 0-padding.\n",
    "            new_tokens = tf.where(done, tf.constant(0, dtype=tf.int64), new_tokens)\n",
    "\n",
    "            # Collect the generated tokens\n",
    "            result_tokens.append(new_tokens)\n",
    "\n",
    "            if tf.executing_eagerly() and tf.reduce_all(done): #if there running excuation or all sentence in that word = 0 \n",
    "                break\n",
    "\n",
    "        # Convert the list of generates token ids to a list of strings.\n",
    "        result_tokens = tf.concat(result_tokens, axis=-1)\n",
    "        result_text = self.tokens_to_text(result_tokens)\n",
    "\n",
    "        if return_attention:\n",
    "            attention_stack = tf.concat(attention, axis=1)\n",
    "            return {'text': result_text, 'attention': attention_stack}\n",
    "        else:\n",
    "            return {'text': result_text}\n",
    "    @tf.function(input_signature=[tf.TensorSpec(dtype=tf.string, shape=[None])])\n",
    "    def tf_translate(self, input_text):\n",
    "        return self.translate(input_text)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wz5dk7BdNz6E",
    "outputId": "204127bd-8cfa-4911-ce7b-cb59b1a1ea3d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:21: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
     ]
    }
   ],
   "source": [
    "translator = Translator(\n",
    "    encoder=train_translator.encoder,\n",
    "    decoder=train_translator.decoder,\n",
    "    input_text_processor=input_tokenizer,\n",
    "    output_text_processor=output_tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 20000)\n",
      "(1, 1, 20000)\n",
      "(1, 1, 20000)\n",
      "(1, 1, 20000)\n",
      "(1, 1, 20000)\n",
      "(1, 1, 20000)\n",
      "(1, 1, 20000)\n",
      "(1, 1, 20000)\n",
      "(1, 1, 20000)\n",
      "(1, 1, 20000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as encoder_9_layer_call_fn, encoder_9_layer_call_and_return_conditional_losses, decoder_5_layer_call_fn, decoder_5_layer_call_and_return_conditional_losses, embedding_14_layer_call_fn while saving (showing 5 of 24). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: lstmQA/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: lstmQA/assets\n"
     ]
    }
   ],
   "source": [
    "tf.saved_model.save(translator, 'lstmQA',\n",
    "                    signatures={'serving_default': translator.tf_translate})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3g9cgw9cOTGS",
    "outputId": "223632f2-90eb-4f0a-b69f-8a924f8e93d0"
   },
   "outputs": [
    {
     "ename": "UnknownError",
     "evalue": "Exception encountered when calling layer \"lstm_11\" (type LSTM).\n\nFail to find the dnn implementation. [Op:CudnnRNN]\n\nCall arguments received:\n  • inputs=tf.Tensor(shape=(2, 350, 256), dtype=float32)\n  • mask=None\n  • training=False\n  • initial_state=None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-102-2decd55e3e83>\u001b[0m in \u001b[0;36mtranslate\u001b[0;34m(self, input_text, max_length, return_attention, temperature)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0minput_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_text_processor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0menc_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mdec_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menc_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-85-e6733a49af69>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, tokens, state)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         whole_seq_output, final_memory_state, final_carry_state = self.lstm(\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0mvectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         )  # pass the embedding vector to the GRU layer\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnknownError\u001b[0m: Exception encountered when calling layer \"lstm_11\" (type LSTM).\n\nFail to find the dnn implementation. [Op:CudnnRNN]\n\nCall arguments received:\n  • inputs=tf.Tensor(shape=(2, 350, 256), dtype=float32)\n  • mask=None\n  • training=False\n  • initial_state=None"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "input_text = tf.constant([\n",
    "    # 'hace mucho frio aqui.', # \"It's really cold here.\"\n",
    "    # 'Esta es mi vida.', # \"This is my life.\"\"\n",
    "    data['q_c'].iloc[1],\n",
    "    data['q_c'].iloc[1]\n",
    "    # data['answer_text'].iloc[0]\n",
    "])\n",
    "\n",
    "result = translator.translate(\n",
    "    input_text = input_text)\n",
    "\n",
    "print(result['text'][0].numpy().decode())\n",
    "print(result['text'][1].numpy().decode())\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "id": "2ezNxWirfDVz",
    "outputId": "e7c20b6b-aae5-442d-d331-aa38a9a0be71"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CLS in what country is normandy located SEP the normans norman nourmands french normands latin normanni were the people who in the 10th and 11th centuries gave their name to normandy a region in france they were descended from norse norman comes from norseman raiders and pirates from denmark iceland and norway who under their leader rollo agreed to swear fealty to king charles iii of west francia through generations of assimilation and mixing with the native frankish and romangaulish populations their descendants would gradually merge with the carolingianbased cultures of west francia the distinct cultural and ethnic identity of the normans emerged initially in the first half of the 10th century and it continued to evolve over the succeeding centuries'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['q_c'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "BpQhmrjY2QXf"
   },
   "outputs": [],
   "source": [
    "reloaded = tf.saved_model.load('lstmQAcp')\n",
    "# result = reloaded.tf_translate(three_input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "input_text = tf.constant([\n",
    "    # 'hace mucho frio aqui.', # \"It's really cold here.\"\n",
    "    # 'Esta es mi vida.', # \"This is my life.\"\"\n",
    "    data['q_c'].iloc[0],\n",
    "    data['q_c'].iloc[0],\n",
    "  \n",
    "    # data['answer_text'].iloc[0]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "denmark\n",
      "denmark\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'sss france eee'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result =reloaded.tf_translate(\n",
    "    input_text = input_text)\n",
    "print(result['text'][0].numpy().decode())\n",
    "print(result['text'][1].numpy().decode())\n",
    "print()\n",
    "data['answer_text'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "attentionQA.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
